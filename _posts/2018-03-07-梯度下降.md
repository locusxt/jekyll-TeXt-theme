---
published: true
key: GradientDescent
tags: ai
---
## 符号表示
样本个数: m  
特征个数: n  
第i个样本: $x^{(i)}, y^{(i)}$  
第i个样本的第j个特征: $ x^{(i)}_{j} $  

hypothesis: $h_{\theta}(x) = \theta_{0} + \theta_{1}x$（假设函数）  

cost function: $J(\theta) = \frac{1} {2m} \sum {(h_{\theta}(x^{(i)})-y^{(i)}})^{2}$（注意sigma）  

学习率: $\alpha$  

## 线性回归
用梯度下降求解：
$\theta_{0} := \theta_{0} - \alpha\frac {\partial {J(\theta)}} {\theta_{0}}$  
$\theta_{1}$也是类似的  
**关键点：** 所有的参数需要**同时更新**，否则不是梯度下降

## 小技巧
### 1. Feature Scaling
所有特征的范围应该相近，否则收敛慢  
一种归一化方式：$x_{new} = \frac {x - mean} {x_{max} - x_{min}}$

多项式回归，可以生成一些高次的项，归一化之后再做线性回归。
### 2. learning rate
学习率太大会导致难以收敛，太小会导致收敛慢  
通过绘制**学习率与cost function**之间的关系，判断当前的学习率是否正常工作，如cost上升意味着学习率过大

## Normal Equation 正规方程
某些情况下，可以直接求解析解

以多元线性回归为例：  
$Y = X \theta$Y是m\*1的矩阵，X是m\*(n+1)的矩阵（第0个特征都为1），$\theta$是(n+1)\*1的矩阵    
$X^T Y = X^T X \theta$ 使得等式右边两项乘积变为一个方阵    
$(X^T X)^{-1} X^T Y = \theta$    

正规方程的特点在于：
1. 不用确定学习率
2. 不用迭代
3. 需要计算逆，复杂度$O(n^3)$。梯度下降复杂度$O(kn^2)$
4. 一般n小于10000时，正规方程会是一个好选择

不可逆的情况：
1. 冗余特征，线性依赖
2. 特征过多，m<=n，删除一些特征或者正则化
