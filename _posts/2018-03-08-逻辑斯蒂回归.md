---
published: true
key: logisticsreg
tags: ai
---
## 定义
sigmoid function = logistics function  
$g(z) = \frac {1} {1+e^{-z}}$，值域是(0,1)。 

sigmoid函数有重要性质： $g'(z) = g(z)*(1-g(z))$
 
二分类问题的假设函数：$h_{\theta}(x) = g(\theta^{T}x)$。实际上$h_{\theta}(x)$能表示特征为x时，$y=1$的概率  

## 决策边界 decision boundary
如果在$g(z)>0.5$时，预测y=1。只需要$z>0$即可  

$\theta^{T} X = 0$构成的平面称为决策边界，边界的一边表示y=1，一边表示y=0  
决策边界是假设函数的属性，而不是训练集的属性。训练集可以帮助确定$\theta$的取值，而$\theta$的取值决定了决策边界  
决策边界具体形式与$\theta^{T} X$有关  

## 代价函数
$J(\theta) = \frac {1}{m} \sum{cost({\theta}(x), y)}$  
$cost(h_{\theta}(x), y) = -log(h_{\theta}(x)) \; if y=1$  
$cost(h_{\theta}(x), y) = -log(1- h_{\theta}(x)) \; if y = 0$  

$cost(h_{\theta}(x), y) = -y * log(h_{\theta}(x)) - (1-y) * log(1- h_{\theta}(x))$   
凸的代价函数，有助于利用梯度下降找到全局最优解

## 梯度下降

$\theta_j := \theta_j - \alpha * \frac {\partial J(\theta)} {\partial \theta_j}$  

利用sigmoid函数求导的性质，容易求得  

$\frac {\partial J(\theta)} {\partial \theta_j} = \frac {1}{m} * \sum(h_{\theta}(x) - y)x_j$  

这与线性回归中梯度下降更新参数，是完全一致的
